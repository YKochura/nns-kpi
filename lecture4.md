class: middle, center, title-slide
# –ù–µ–π—Ä–æ–Ω–Ω—ñ –º–µ—Ä–µ–∂—ñ

–õ–µ–∫—Ü—ñ—è 4: –ù–∞–≤—á–∞–Ω–Ω—è –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂

<br><br>
–ö–æ—á—É—Ä–∞ –Æ—Ä—ñ–π –ü–µ—Ç—Ä–æ–≤–∏—á<br>
[iuriy.kochura@gmail.com](mailto:iuriy.kochura@gmail.com) <br>
<a href="https://t.me/y_kochura">@y_kochura</a> <br>


---

class:  black-slide, 
background-image: url(./figures/lec1/nn.jpg)
background-size: cover

# –°—å–æ–≥–æ–¥–Ω—ñ

.larger-x[ <p class="shadow" style="line-height: 200%;">–Ø–∫ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏? <br>

üéôÔ∏è –ü–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ <br>
üéôÔ∏è –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ <br>
üéôÔ∏è –úi–Ωi-–ø–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ <br>
üéôÔ∏è I–º–ø—É–ª—å—Å <br>  
üéôÔ∏è –ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –º–µ—Ç–æ–¥–∏: <br> 
   &nbsp;&nbsp;&nbsp;AdaGrad <br> 
   &nbsp;&nbsp;&nbsp; RMSProp <br> 
   &nbsp;&nbsp;&nbsp; Adam <br> 
</p>]

---


class: blue-slide, middle, center
count: false

.larger-xx[–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤]

---

class: middle

# –ú–æ–¥–µ–ª—å

–•–æ—á–∞ —Ç–µ, —â–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –≥–ª–∏–±–æ–∫–æ—ó –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ, –º–æ–∂–µ –±—É—Ç–∏ —Å–∫–ª–∞–¥–Ω–∏–º, —É —Å–≤–æ—ó–π –æ—Å–Ω–æ–≤—ñ —Ü–µ –ø—Ä–æ—Å—Ç–æ —Ñ—É–Ω–∫—Ü—ñ—ó. –í–æ–Ω–∏ –ø—Ä–∏–π–º–∞—é—Ç—å –¥–µ—è–∫—ñ –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ —Ç–∞ –≥–µ–Ω–µ—Ä—É—é—Ç—å –¥–µ—è–∫—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏.

.center.width-30[![](figures/lec4/func.png)]

.footnote[Credits: NVIDIA]

---

class: middle

# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –º–æ–¥–µ–ª—ñ

.center.width-100[![](figures/lec4/modelComp.png)]

.footnote[Credits: NVIDIA]

???
–ù–∞–≤—á–µ–Ω–∞ –º–µ—Ä–µ–∂–∞ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –¥–≤–æ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤:
- –û–ø–∏—Å –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –Ω–µ–Ω–∞–≤—á–µ–Ω–æ—ó –º–µ—Ä–µ–∂—ñ.
- –í–∞–≥–∏, —è–∫—ñ –±—É–ª–∏ "–≤–∏–≤—á–µ–Ω—ñ" –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –º–µ—Ä–µ–∂—ñ.

---

class: middle

# –ó–∞–≥–∞–ª—å–Ω–∏–π –ø—Ä–æ—Ü–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂

1. –í–∏–∑–Ω–∞—á—Ç–µ –∑–∞–≤–¥–∞–Ω–Ω—è + –∑–±–µ—Ä—ñ—Ç—å –¥–∞–Ω—ñ
2. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
3. –û–±–µ—Ä—ñ—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
4. –ü–æ–≤—Ç–æ—Ä—ñ—Ç—å —Ü—ñ –∫—Ä–æ–∫–∏:
.smaller-xx[
    4.1. –ü—Ä—è–º–µ –ø–æ—à–µ—Ä–µ–Ω–Ω—è –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö

    4.2 –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ—ó —Ñ—É–Ω–∫—Ü—ñ—é –≤–∏—Ç—Ä–∞—Ç]
.smaller-xx[
    4.3 –ó–≤–æ—Ä–æ—Ç–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è: –æ–±—á–∏—Å–ª—ñ—Ç—å –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ —Ü—ñ–ª—å–æ–≤–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –≤—ñ–¥–Ω–æ—Å–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

    4.4 –û–Ω–æ–≤—ñ—Ç—å –∫–æ–∂–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó]

???
To build a machine learning algorithm, usually you‚Äôd define an architecture (e.g. Logistic regression, Support Vector Machine, Neural Network) and train it to learn parameters. Here is a common training process for neural networks.

Initialization can have a significant impact on convergence in training deep neural networks. Simple initialization schemes have been found to accelerate training, but they require some care to avoid common pitfalls.

Initializing all the weights with zeros leads the neurons to learn the same features during training. 

A too-large initialization leads to exploding gradients. That is, the gradients of the cost with the respect to the parameters are too big. This leads the cost to oscillate around its minimum value.

A too-small initialization leads to vanishing gradients. The gradients of the cost with respect to the parameters are too small, leading to convergence of the cost before it has reached the minimum value.


In machine learning, you start by defining a task and a model. The model consists of an architecture and parameters. For a given architecture, the values of the parameters determine how accurately the model performs the task. But how do you find good values? By defining a loss function that evaluates how well the model performs. The goal is to minimize the loss and thereby to find parameter values that match predictions with reality. This is the essence of training.

---

class: middle

# –ó–∞–≥–∞–ª—å–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç–∏

- –ï–≤–∫–ªi–¥–æ–≤–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å (L2 –≤—Ç—Ä–∞—Ç–∏)
- –°–µ—Ä–µ–¥–Ω—å–æ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞ –ø–æ—Ö–∏–±–∫–∞ (MSE)
- –ú–∞–Ω—Ö–µ—Ç—Ç–µ–Ω—Å—å–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å (L1 –≤—Ç—Ä–∞—Ç–∏)
- –°–µ—Ä–µ–¥–Ω—è –∞–±—Å–æ–ª—é—Ç–Ω–∞ –ø–æ—Ö–∏–±–∫–∞(MAE)
- –ü–µ—Ä–µ—Ö—Ä–µ—Å–Ω–∞ –≤—Ç—Ä–∞—Ç–∞ –µ–Ω—Ç—Ä–æ–øi—ó

???
In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event. 

---

class: middle

.width-100[![](figures/lec4/euclidian.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

???
It is a distance measure that best can be explained as the length of a segment connecting two points.

---

class: middle

# –ï–≤–∫–ªi–¥–æ–≤–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å (L2 –≤—Ç—Ä–∞—Ç–∏)

$$d(\hat y,y) = \sqrt{\sum\_{i=1}^n \left(\hat y^{(i)} - y^{(i)}\right)^2} = \left \Vert \hat y - y \right \Vert\_2$$
–¥–µ $n$ ‚Äì –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.

???
Euclidean distance works great when you have low-dimensional data and the magnitude of the vectors is important to be measured.

Although it is a common distance measure, Euclidean distance is not scale in-variant which means that distances computed might be skewed depending on the units of the features. Typically, one needs to normalize the data before using this distance measure.

---

class: middle

# –°–µ—Ä–µ–¥–Ω—å–æ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞ –ø–æ—Ö–∏–±–∫–∞ (MSE)

$$\begin{aligned}
\mathcal{L}(\hat y^{(i)}, y^{(i)}) &= (\hat y^{(i)} - y^{(i)})^2 \\\\
\mathcal{J}(\hat y,y) &= \frac{1}{n} \sqrt{\sum\_{i=1}^n \left(\hat y^{(i)} - y^{(i)}\right)^2} = \frac{1}{n} \left \Vert \hat y - y \right \Vert\_2
\end{aligned}$$
–¥–µ $n$ ‚Äì –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.

---

class: middle

.width-100[![](figures/lec4/ManhattanDistance.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

---

class: middle

.width-100[![](figures/lec4/ManhattanDistance2.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

???
The Manhattan distance, often called Taxicab distance or City Block distance, calculates the distance between real-valued vectors. 

The distance between two points measured along axes at right angles.

---

class: middle

# –ú–∞–Ω—Ö–µ—Ç—Ç–µ–Ω—Å—å–∫–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å (L1 –≤—Ç—Ä–∞—Ç–∏)

$$d(\hat y,y) = \sum\_{i=1}^n |\hat y^{(i)} - y^{(i)}| = \left \Vert \hat y - y \right \Vert\_1$$
–¥–µ $n$ ‚Äì –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.

---

class: middle

# –°–µ—Ä–µ–¥–Ω—è –∞–±—Å–æ–ª—é—Ç–Ω–∞ –ø–æ—Ö–∏–±–∫–∞(MAE)

$$\begin{aligned}
\mathcal{L}(\hat y^{(i)}, y^{(i)}) &= |\hat y^{(i)} - y^{(i)}| \\\\
\mathcal{J}(\hat y,y) &= \frac{1}{n} \sum_{i=1}^n |\hat y^{(i)} - y^{(i)}| = \frac{1}{n} \left \Vert \hat y - y \right \Vert_1
\end{aligned}$$
–¥–µ $n$ ‚Äì –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.

---

class: middle

# –ü–µ—Ä–µ—Ö—Ä–µ—Å–Ω–∞ –≤—Ç—Ä–∞—Ç–∞ –µ–Ω—Ç—Ä–æ–øi—ó (–±—ñ–Ω–∞—Ä–Ω–∞)

$$\begin{aligned}
\mathcal{L}(\hat y^{(i)},y^{(i)}) &= - y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \\\\
\mathcal{J}(\hat y,y)  &= - \frac{1}{n} \sum\_{i=1}^n \Big[ y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)}) \Big]
\end{aligned}$$
–¥–µ $n$ ‚Äì –∑–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –ø—Ä–∏–∫–ª–∞–¥—ñ–≤.

---

class: middle

## 9 –º–µ—Ç—Ä–∏–∫ –≤—ñ–¥—Å—Ç–∞–Ω—ñ –≤ –Ω–∞—É—Ü—ñ –ø—Ä–æ –¥–∞–Ω—ñ

.center.width-70[![](figures/lec4/distances.png)]

.footnote[Slide source: [9 Distance Measures in Data Science](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa)]

???
Cosine similarity has often been used as a way to counteract Euclidean distance‚Äôs problem with high dimensionality. The cosine similarity is simply the cosine of the angle between two vectors.

One main disadvantage of cosine similarity is that the magnitude of vectors is not taken into account, merely their direction. In practice, this means that the differences in values are not fully taken into account. 

We use cosine similarity often when we have high-dimensional data and when the magnitude of the vectors is not of importance. 


Hamming distance is the number of values that are different between two vectors. It is typically used to compare two binary strings of equal length. As you might expect, hamming distance is difficult to use when two vectors are not of equal length.Typical use cases include error correction/detection when data is transmitted over computer networks. It can be used to determine the number of distorted bits in a binary word as a way to estimate error.

Although Manhattan distance seems to work okay for high-dimensional data, it is a measure that is somewhat less intuitive than euclidean distance, especially when using in high-dimensional data. Moreover, it is more likely to give a higher distance value than euclidean distance since it does not the shortest path possible. This does not necessarily give issues but is something you should take into account. When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes. 

Chebyshev distance is defined as the greatest of difference between two vectors along any coordinate dimension. In other words, it is simply the maximum distance along one axis.

The Jaccard index (or Intersection over Union) is a metric used to calculate the similarity and diversity of sample sets. It is the size of the intersection divided by the size of the union of the sample sets.

Haversine distance is the distance between two points on a sphere given their longitudes and latitudes.


The S√∏rensen-Dice index is very similar to Jaccard index in that it measures the similarity and diversity of sample sets.

---


.width-100[![](figures/lec4/optimization.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

---

class: blue-slide, middle, center
count: false

.larger-xx[–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π–Ω—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏]

---


class: middle

# –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ & –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

- –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —î –Ω–∞–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ vs. –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

.grid[
.kol-1-2[
**–ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏**
.smaller-xx[
- –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –ø–æ–¥—ñ–ª—É –¥–∞—Ç–∞—Å–µ—Ç—É –Ω–∞–≤—á–∞–Ω–Ω—è/–≤–∞–ª—ñ–¥–∞—Ü—ñ—è/—Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
- –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
- –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤ —É –ù–ú
- –ö—ñ–ª—å–∫—ñ—Å—Ç—å –±–ª–æ–∫—ñ–≤ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –≤ –∫–æ–∂–Ω–æ–º—É —à–∞—Ä—ñ
- –†–æ–∑–º—ñ—Ä –ø–∞–∫–µ—Ç—É

.center[$\vdots$]
]]
.kol-1-2[
**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏**
.smaller-xx[
- –í–∞–≥–∏ —Ç–∞ –∑—Å—É–≤–∏ –ù–ú
- –¶–µ–Ω—Ç—Ä–æ—ó–¥–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ —É –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
]]]

.footnote[Credits:: [Kizito Nyuytiymbiy](https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac)]

???
Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. The prefix ‚Äòhyper_‚Äô suggests that they are ‚Äòtop-level‚Äô parameters that control the learning process and the model parameters that result from it.

Parameters on the other hand are internal to the model. That is, they are learned or estimated purely from the data during training as the algorithm used tries to learn the mapping between the input features and the labels or targets.

---

class: middle

# –ó–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó

## –ú—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—è –µ–º–ø—ñ—Ä–∏—á–Ω–æ–≥–æ —Ä–∏–∑–∏–∫—É (–≤—Ç—Ä–∞—Ç)

$$W\_\*^\{\mathbf{d}} = \arg \min\_W \mathcal{J}(W) = \arg \min\_W \frac{1}{n} \sum\_{i=1}^n \mathcal{L}\left(y^{(i)}, f(\mathbf{x}^{(i)}, W)\right)$$

---

class: black-slide

.width-100[![](figures/lec4/lossSurface.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

---

class: middle

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

–ù–∞–≤—á–∞–Ω–Ω—è –º–∞—Å–∏–≤–Ω–æ—ó –≥–ª–∏–±–æ–∫–æ—ó –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ —î —Ç—Ä–∏–≤–∞–ª–∏–º —Ç–∞ —Å–∫–ª–∞–¥–Ω–∏–º –ø—Ä–æ—Ü–µ—Å–æ–º.

–ü–µ—Ä—à–∏–º –∫—Ä–æ–∫–æ–º –¥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è, –Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂ —î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó:

- –ø–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –≤—Ç—Ä–∞—Ç —Ç–∞ —ñ–Ω—à–∏—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ,
- –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏—Ö –≥—Ä–∞—Ñ—ñ–∫—ñ–≤,
- –ø–æ–∫–∞–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –º–µ—Ä–µ–∂—ñ.

---

background-image: url(figures/lec4/tensorboard.png)

???
TensorBoard provides the visualization and tooling needed for machine learning experimentation: 
- Tracking and visualizing metrics such as loss and accuracy
- Visualizing the model graph (ops and layers)
- Viewing histograms of weights, biases, or other tensors as they change over time
- Projecting embeddings to a lower dimensional space
- Displaying images, text, and audio data
- Profiling TensorFlow programs

---

class: middle, center

.larger-xx[[–î–µ–º–æ](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjYtYasjvnzAhUPCewKHYANDggQFnoECAQQAQ&url=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2Ftensorflow%2Ftensorboard%2Fblob%2Fmaster%2Fdocs%2Ftensorboard_in_notebooks.ipynb&usg=AOvVaw1vAk6hbwao2KtXBNnZ1cez)]

---

class: blue-slide, middle, center
count: false

.larger-xx[–ü–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ 

GD]

---

class: middle

# –ü–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫

–©–æ–± –º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ $\mathcal{J}(W)$, 
**–ø–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫** (GD) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –Ω–∞—Å—Ç—É–ø–Ω–µ –ø—Ä–∞–≤–∏–ª–æ:
$$\begin{aligned}
g\_t &= \frac{1}{n} \sum\_{i=1}^n \nabla\_W \mathcal{L}\left(y^{(i)}, f(\mathbf{x}^{(i)}, W)\right) = \nabla\_W \mathcal{J}(W)\\\\
W\_{t+1} &= W\_t - \alpha g\_t,
\end{aligned}$$
–¥–µ–∫ $\alpha$ &mdash; –∫—Ä–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è.
.center.width-60[![](figures/lec2/gd-good-2.png)]

---

class: middle

# GD

.larger-x[–ù–∞–π–≥—ñ—Ä—à–∏–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –≤ —Å–≤—ñ—Ç—ñ]

.alert[–ü—Ä–∏–º—ñ—Ç–∫–∞: –π–º–æ–≤—ñ—Ä–Ω–æ, –í–∞–º –Ω—ñ–∫–æ–ª–∏ –Ω–µ —Å–ª—ñ–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ, –≤–≤–∞–∂–∞–π—Ç–µ —Ü–µ –±—É–¥—ñ–≤–µ–ª—å–Ω–∏–º –±–ª–æ–∫–æ–º –¥–ª—è —ñ–Ω—à–∏—Ö –º–µ—Ç–æ–¥—ñ–≤.]

.footnote[Credits: Aaron Defazio, Facebook AI Research]

---

class: middle

# –ö—Ä–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è

.center.width-70[![](figures/lec4/lr.png)]

–¢—É—Ç $\gamma = \alpha$ &mdash; –∫—Ä–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è.

.footnote[Credits: Aaron Defazio, Facebook AI Research]

???
The learning rate $\gamma = \alpha$ can be set by the algorithm designer. If we use a learning rate that is too small, it will cause $W$ to update very slowly, requiring more iterations to get a better solution.

Typically we don't have a good estimate of the learning rate. Standard practice is to try a bunch of values on a log scale and use the one that gave the best final result.

Learning rates that are too large cause divergence where the function value (loss) explodes.

The optimal learning rate can change during optimization! Often decreasing it over time is necessary.

---


class: black-slide

.width-100[![](figures/lec4/surface.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

???
The empirical risk is an average loss on the training dataset while the risk is the expected loss on the entire population of data.

There are many challenges in deep learning optimization. Some of the most vexing ones are local minima, saddle points, and vanishing gradients.

---

class: black-slide

.width-100[![](figures/lec4/surface2.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

???
For any objective function $\mathcal{J}(W)$, if the value of $\mathcal{J}(W)$ at $W$ is smaller than the values of $W$ at any other points in the vicinity of $W$, then $\mathcal{J}(W)$ could be a local minimum. If the value of $\mathcal{J}(W)$ at $W$ is the minimum of the objective function over the entire domain, then $\mathcal{J}(W)$ is the global minimum.

---

class: black-slide, middle

.center[
<video loop controls preload="auto" height="600" width="600">
  <source src="./figures/lec4/follow-slope.mp4" type="video/mp4">
</video>
]


.footnote[Slide source: [Gradient descent, how neural networks learn](https://www.3blue1brown.com/lessons/gradient-descent)]

---

class: black-slide, middle

.center[
<video loop controls preload="auto" height="600" width="600">
  <source src="./figures/lec4/balls-rolling-down.mp4" type="video/mp4">
</video>
]


.footnote[Slide source: [Gradient descent, how neural networks learn](https://www.3blue1brown.com/lessons/gradient-descent)]

---



class: middle

.center[
<video loop controls preload="auto" height="600" width="600">
  <source src="./figures/lec4/opt-gd.mp4" type="video/mp4">
</video>
]

---

class: blue-slide, middle, center
count: false

.larger-xx[C—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ 

SGD]

---


class: middle

# SGD

–©–æ–± –∑–º–µ–Ω—à–∏—Ç–∏ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω—É —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å, **—Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫** (SGD) –ø–æ–ª—è–≥–∞—î –≤ –æ–Ω–æ–≤–ª–µ–Ω–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É
$$\begin{aligned}
\ell^{(i)} &= \mathcal{L}\left(y^{(i)}, f(\mathbf{x}^{(i)}, W)\right) \\\\
g^{(i)}\_t &= \nabla\_W \ell^{(i)} \\\\
W\_{t+1} &= W\_t - \alpha g^{(i)}\_t
\end{aligned}$$


???
In deep learning, the objective function is usually the average of the loss functions for each example in the training dataset. Given a training dataset of $n$ examples, we assume that $\ell^{(i)}$ is the loss function with respect to the training example of index $i$, where $W$ is the parameter vector.

---

class: middle

# –ü–µ—Ä–µ–≤–∞–≥–∏ SGD

- –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ –∑–Ω–∞—á–Ω–æ –ª–µ–≥—à–µ (–ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ —Ä–æ–∑–º—ñ—Ä—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö), —Ç–æ–º—É –í–∏ —á–∞—Å—Ç–æ –º–æ–∂–µ—Ç–µ –∑—Ä–æ–±–∏—Ç–∏ —Ç–∏—Å—è—á—ñ –∫—Ä–æ–∫—ñ–≤ SGD –∑–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –∫—Ä–æ–∫—É GD.

- –£ —Å–µ—Ä–µ–¥–Ω—å–æ–º—É —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç —î —Ö–æ—Ä–æ—à–æ—é –æ—Ü—ñ–Ω–∫–æ—é –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞.

- –®—É–º –º–æ–∂–µ –ø–µ—Ä–µ—à–∫–æ–¥–∂–∞—Ç–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π–Ω–æ–º—É —Å—Ö–æ–¥–∂–µ–Ω–Ω—é –¥–æ –ø–æ–≥–∞–Ω–∏—Ö –ª–æ–∫–∞–ª—å–Ω–∏—Ö –º—ñ–Ω—ñ–º—É–º—ñ–≤.
.footnote[Credits: Aaron Defazio, Facebook AI Research]

---

class: blue-slide, middle, center
count: false

.larger-xx[–ú—ñ–Ω—ñ-–ø–∞–∫–µ—Ç–∏]

---

class: middle

# –ú—ñ–Ω—ñ-–ø–∞–∫–µ—Ç–∏

–û–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç –¥–ª—è –º—ñ–Ω—ñ-–ø–∞–∫–µ—Ç—ñ–≤ —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
$$
\begin{aligned}
g^{(k)}\_t &= \frac{1}{B} \sum\_{i=1}^B \nabla\_W \mathcal{L}\left(y\_k^{(i)}, f(\mathbf{x}\_k^{(i)}, W)\right) \\\\
W\_{t+1} &= W\_t - \alpha g^{(k)}\_t,
\end{aligned}
$$
–¥–µ $k$ &mdash; —ñ–Ω–¥–µ–∫—Å –º—ñ–Ω—ñ-–ø–∞–∫–µ—Ç–∞.

- –ó–±—ñ–ª—å—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –ø–∞–∫–µ—Ç—É $B$ –∑–º–µ–Ω—à—É—î –¥–∏—Å–ø–µ—Ä—Å—ñ—é –æ—Ü—ñ–Ω–æ–∫ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ —Ç–∞ –∑–∞–±–µ–∑–ø–µ—á—É—î –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –ø–∞–∫–µ—Ç–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏.
- –í–∑–∞—î–º–æ–∑–≤'—è–∑–æ–∫ –º—ñ–∂ $B$ —ñ $\alpha$ –≤—Å–µ —â–µ –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∏–π.

---


class: blue-slide, middle, center
count: false

.larger-xx[–Ü–º–ø—É–ª—å—Å]

---

class: middle

# –Ü–º–ø—É–ª—å—Å

SGD + –Ü–º–ø—É–ª—å—Å = –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –º–µ—Ç–æ–¥ *–≤–∞–∂–∫–æ—ó –∫—É–ª—ñ*

$$
\begin{aligned}
p\_{t + 1} &= \beta\_t p\_{t} + \nabla \ell^{(i)} (W\_t)\\\\
W\_{t+1} &= W\_t - \alpha\_t p\_{t + 1}
\end{aligned}
$$

–ü—Ä–∞–≤–∏–ª–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è:

$$
\begin{aligned}
W\_{t+1} &= W\_t - \alpha\_t \nabla \ell^{(i)} (W\_t) + \beta\_t \left(W\_{t} - W\_{t-1}\right)
\end{aligned}
$$

**–ö–ª—é—á–æ–≤–∞ —ñ–¥–µ—è:** –ù–∞—Å—Ç—É–ø–Ω–∏–π –∫—Ä–æ–∫ —Å—Ç–∞—î –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—î—é –Ω–∞–ø—Ä—è–º–∫—É –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∫—Ä–æ–∫—É —Ç–∞ –Ω–æ–≤–æ–≥–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞.

???
Momentum replaces gradients with a leaky average over past gradients. This accelerates convergence significantly. Momentum usually speeds up the learning with a very minor implementation change. 

---


<iframe class="iframemomentum" src="https://distill.pub/2017/momentum/" scrolling="no" frameborder="no"  style="position:absolute; top:-165px; left: -25px; width:950px; height: 600px"></iframe>

.footnote[Credits:  Distill, [Why Momentum Really Works?](https://distill.pub/2017/momentum/)]

???
**Intuition.** The optimization process resembles a **heavy ball** rolling down a hill. The ball has **momentum**, so it doesn‚Äôt change direction immediately when it encounters changes to the landscape!

---

class: middle, black-slide

.center[
<video loop controls preload="auto" height="500" width="600">
  <source src="./figures/lec4/sgd-momentum.mp4" type="video/mp4">
</video>
]

.footnote[Image credits: Kosta Derpanis, [Deep Learning in Computer Vision](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html), 2018]

---

class: middle

# –ü–µ—Ä–µ–≤–∞–≥–∏

SGD –∑ —ñ–º–ø—É–ª—å—Å–æ–º –º–∞—î **—Ç—Ä–∏** —Ö–æ—Ä–æ—à—ñ –≤–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:
- –≤—ñ–Ω –º–æ–∂–µ –ø—Ä–æ–π—Ç–∏ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ñ –±–∞—Ä'—î—Ä–∏
- –ø—Ä–∏—Å–∫–æ—Ä—é—î—Ç—å—Å—è, —è–∫—â–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç –Ω–µ —Å–∏–ª—å–Ω–æ –∑–º—ñ–Ω—é—î—Ç—å—Å—è
- –≥–∞—Å–∏—Ç—å –∫–æ–ª–∏–≤–∞–Ω–Ω—è —É –≤—É–∑—å–∫–∏—Ö –¥–æ–ª–∏–Ω–∞—Ö

---

class: middle

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –∞—Å–ø–µ–∫—Ç–∏ —ñ–º–ø—É–ª—å—Å—É

–ü–æ —Å—É—Ç—ñ, —Ü–µ **¬´–±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏–π –æ–±—ñ–¥¬ª**, –º–∞–π–∂–µ –≤ —É—Å—ñ—Ö —Å–∏—Ç—É–∞—Ü—ñ—è—Ö **SGD + —ñ–º–ø—É–ª—å—Å** –∫—Ä–∞—â–µ, –Ω—ñ–∂ SGD, —ñ –¥—É–∂–µ —Ä—ñ–¥–∫–æ –≥—ñ—Ä—à–µ!

## –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:

$\beta = 0.9$ or $0.99$ –º–∞–π–∂–µ –∑–∞–≤–∂–¥–∏ –ø—Ä–∞—Ü—é—Ç—å –¥–æ–±—Ä–µ. –Ü–Ω–æ–¥—ñ –º–æ–∂–Ω–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–µ–≤–µ–ª–∏–∫—ñ –ø–µ—Ä–µ–≤–∞–≥–∏, –Ω–∞–ª–∞—à—Ç—É–≤–∞–≤—à–∏ –π–æ–≥–æ.

–ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–æ–∑–º—ñ—Ä—É –∫—Ä–æ–∫—É ($\alpha$) –∑–∞–∑–≤–∏—á–∞–π –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–º–µ–Ω—à—É–≤–∞—Ç–∏, –∫–æ–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä —ñ–º–ø—É–ª—å—Å—É –∑–±—ñ–ª—å—à—É—î—Ç—å—Å—è, —â–æ–± –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å.

.footnote[Credits: Aaron Defazio, Facebook AI Research]

---


class: middle, center

.larger-xx[[–î–µ–º–æ](https://www.deeplearning.ai/ai-notes/optimization/)]

---

class: blue-slide, middle, center
count: false

.larger-xx[–ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –º–µ—Ç–æ–¥–∏]

---

class: middle

# –ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –º–µ—Ç–æ–¥–∏  

–í–µ–ª–∏—á–∏–Ω–∞ –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ —á–∞—Å—Ç–æ —Å–∏–ª—å–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –º—ñ–∂ —à–∞—Ä–∞–º–∏, —Ç–æ–º—É –≥–ª–æ–±–∞–ª—å–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–∂–µ –Ω–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –Ω–∞–ª–µ–∂–Ω–∏–º —á–∏–Ω–æ–º.

*–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–¥–µ—è:* –ó–∞–º—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ–± –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –æ–¥–Ω–∞–∫–æ–≤—É —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ—ó –≤–∞–≥–∏ –≤ –Ω–∞—à—ñ–π –º–µ—Ä–µ–∂—ñ, **–ø—ñ–¥—Ç—Ä–∏–º—É–π—Ç–µ –æ—Ü—ñ–Ω–∫—É –∫—Ä–∞—â–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ –æ–∫—Ä–µ–º–æ –¥–ª—è –∫–æ–∂–Ω–æ—ó –≤–∞–≥–∏**.

–¢–æ—á–Ω–∏–π —Å–ø–æ—Å—ñ–± –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó –¥–æ —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∞–ª–≥–æ—Ä–∏—Ç–º—É, –∞–ª–µ –±—ñ–ª—å—à—ñ—Å—Ç—å –º–µ—Ç–æ–¥—ñ–≤ –∞–±–æ **–ø—Ä–∏—Å—Ç–æ—Å–æ–≤—É—é—Ç—å—Å—è** –¥–æ **–¥–∏—Å–ø–µ—Ä—Å—ñ—ó –≤–∞–≥**, –∞–±–æ –¥–æ **–ª–æ–∫–∞–ª—å–Ω–æ—ó –∫—Ä–∏–≤–∏–∑–Ω–∏** –ø—Ä–æ–±–ª–µ–º–∏.

.footnote[Credits: Aaron Defazio, Facebook AI Research]

---


class: middle

## AdaGrad

–ó–º–µ–Ω—à–µ–Ω–Ω—è –º–∞—Å—à—Ç–∞–±—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç–Ω–∏–π –∫–æ—Ä—ñ–Ω—å —ñ–∑ —Å—É–º–∏ –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ —É—Å—ñ—Ö –π–æ–≥–æ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å.

$$\begin{aligned}
r\_t  &=  r\_{t-1} + g\_t \odot g\_t \\\\
W\_{t+1} &= W\_t - \frac{\alpha}{\varepsilon + \sqrt{r\_t}} \odot g\_t
\end{aligned}$$

- AdaGrad –ø–æ–∑–±–∞–≤–ª—è—î –≤—ñ–¥ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –≤—Ä—É—á–Ω—É –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞—Ç–∏ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è.
  –ë—ñ–ª—å—à—ñ—Å—Ç—å —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å $\alpha=0.01$ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.
- –î–æ–±—Ä–µ, –∫–æ–ª–∏ —Ü—ñ–ª—å–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è –æ–ø—É–∫–ª–∞.
- $r_t$ –Ω–µ–æ–±–º–µ–∂–µ–Ω–æ –∑—Ä–æ—Å—Ç–∞—î –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è, —â–æ –º–æ–∂–µ —Å–ø—Ä–∏—á–∏–Ω–∏—Ç–∏ –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫—Ä–æ–∫—É —Ç–∞ –∑—Ä–µ—à—Ç–æ—é —Å—Ç–∞—Ç–∏ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–æ –º–∞–ª–∏–º.
- $\varepsilon$ –∞–¥–∏—Ç–∏–≤–Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞, —è–∫–∞ –≥–∞—Ä–∞–Ω—Ç—É—î, —â–æ –º–∏ –Ω–µ –¥—ñ–ª–∏–º–æ –Ω–∞ 0.


???
The variable $r\_t$ accumulates past gradient variance.

---


class: middle

## RMSProp

–¢–µ —Å–∞–º–µ, —â–æ AdaGrad, –∞–ª–µ –Ω–∞–∫–æ–ø–∏—á—É—î –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–∞–ª—å–Ω–æ —Å–ø–∞–¥–Ω–µ —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞.

**–ö–ª—é—á–æ–≤–∞ —ñ–¥–µ—è:** –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–∞ —Å–µ—Ä–µ–¥–Ω—å–æ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞

$$\begin{aligned}
r\_t  &=  \rho r\_{t-1} + (1-\rho) g\_t \odot g\_t \\\\
W\_{t+1} &= W\_t - \frac{\alpha}{\varepsilon + \sqrt{r\_t}} \odot g\_t
\end{aligned}$$

- –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à–∏–π, –∫–æ–ª–∏ —Ü—ñ–ª—å–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è –Ω–µ —î –æ–ø—É–∫–ª–æ—é.

???
An alternative is to use a leaky average in the same way we used in the momentum method, for some parameter $\rho>0$.

---


class: middle

## Adam: RMSprop –∑ —ñ–º–ø—É–ª—å—Å–æ–º

.smaller-xx[‚ÄúAdaptive Moment Estimation‚Äù]

–ü–æ–¥—ñ–±–Ω–æ –¥–æ RMSProp –∑ —ñ–º–ø—É–ª—å—Å–æ–º, –∞–ª–µ –∑ —É–º–æ–≤–∞–º–∏ –∫–æ—Ä–µ–∫—Ü—ñ—ó –∑—Å—É–≤—É –¥–ª—è –ø–µ—Ä—à–æ–≥–æ —Ç–∞ –¥—Ä—É–≥–æ–≥–æ –º–æ–º–µ–Ω—Ç—ñ–≤.

$$\begin{aligned}
p\_t  &=  \rho\_1 p\_{t-1} + (1-\rho\_1) g\_t \\\\
\hat{p}\_t &= \frac{p\_t}{1-\rho\_1^t} \\\\
r\_t  &=  \rho\_2 r\_{t-1} + (1-\rho\_2) g\_t \odot g\_t \\\\
\hat{r}\_t &= \frac{r\_t}{1-\rho\_2^t} \\\\
W\_{t+1} &= W\_t - \alpha \frac{\hat{p}\_t}{\varepsilon+\sqrt{\hat{r}\_t}}
\end{aligned}$$

- –•–æ—Ä–æ—à—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º $\rho\_1=0.9$ —ñ $\rho\_2=0.999$.
- –ü–æ–¥—ñ–±–Ω–æ –¥–æ —Ç–æ–≥–æ, —è–∫ —ñ–º–ø—É–ª—å—Å –ø–æ–∫—Ä–∞—â—É—î SGD, –≤—ñ–Ω —Ç–∞–∫–æ–∂ –ø–æ–∫—Ä–∞—â—É—î RMSProp.

???
Adam is one of the **default optimizers** in deep learning, along with SGD with momentum.

---

# –ü—Ä–∞–∫—Ç–∏—á–Ω–∞ —Å—Ç–æ—Ä–æ–Ω–∞

–î–ª—è –ø–æ–≥–∞–Ω–æ –æ–±—É–º–æ–≤–ª–µ–Ω–∏—Ö –∑–∞–¥–∞—á Adam —á–∞—Å—Ç–æ –Ω–∞–±–∞–≥–∞—Ç–æ –∫—Ä–∞—â–∏–π, –Ω—ñ–∂ SGD.

–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ Adam –∑–∞–º—ñ—Å—Ç—å RMSprop –∑–∞–≤–¥—è–∫–∏ –æ—á–µ–≤–∏–¥–Ω–∏–º –ø–µ—Ä–µ–≤–∞–≥–∞–º —ñ–º–ø—É–ª—å—Å—É.

## –ê–ª–µ, Adam –ø–æ–≥–∞–Ω–æ –≤–∏–≤—á–µ–Ω–∏–π —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–æ —Ç–∞ –º–∞—î –≤—ñ–¥–æ–º—ñ –Ω–µ–¥–æ–ª—ñ–∫–∏::

- –ó–æ–≤—Å—ñ–º –Ω–µ —Å—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞ –¥–µ—è–∫–∏—Ö –ø—Ä–æ—Å—Ç–∏—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö –∑–∞–¥–∞—á!
- –î–∞—î –≥—ñ—Ä—à—É –ø–æ–º–∏–ª–∫—É —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –∑–∞–¥–∞—á –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, ImageNet)
- –ü–æ—Ç—Ä—ñ–±–Ω–æ –±—ñ–ª—å—à–µ –ø–∞–º'—è—Ç—ñ, –Ω—ñ–∂ SGD
- –ú–∞—î 2 –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–º–µ–Ω—Ç—ñ–≤, —Ç–æ–º—É –º–æ–∂–µ –∑–Ω–∞–¥–æ–±–∏—Ç–∏—Å—è –¥–µ—è–∫–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è

.footnote[Credits: Aaron Defazio, Facebook AI Research]

---

class: middle, center

.larger-xx[[–î–µ–º–æ](https://www.deeplearning.ai/ai-notes/optimization/)]

---

class: middle, center

.larger-xx[[demo - losslandscape](https://losslandscape.com/explorer)]

---


class: end-slide, center
count: false

.larger-xxxx[üèÅ]


